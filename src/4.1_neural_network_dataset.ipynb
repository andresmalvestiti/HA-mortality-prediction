{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import print_column_with_nan\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET_PATH = '../data-new/data-without-troponina.csv'\n",
    "df = pd.read_csv(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear un dataset sin feature selection para entrenar algunas redes neuronales pequeñas. Aplicaremos un split en los datos para obtener sets de train, validacion y test. Luego haremos una imputacion en los datos y finalmente un escalado de las variables. Vamos a usar el mismo codigo que en los notebooks anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumptions\n",
    "NUMERICAL_FEATURES = ['EDAD','PESO Kg', 'ALTURA cm', 'CREAT', 'GLUCEMIA INGR', 'GB', 'TAS INGRESO', 'FC INGRESO']\n",
    "NUMERICAL_CONTINOUS = NUMERICAL_FEATURES\n",
    "\n",
    "CATEGORICAL_MULTI_CLASS_FEATURES = ['KILLIP Ingreso', 'FUNCION VENTRICULAR IZQ', 'DIAGNOSTICO', 'NUMERO VASOS', 'Peor KILLIP']\n",
    "CATEGORICAL_BINARY_FEATURES = ['NUMERO VASOS', 'SEX0', 'HTA', 'DBT', 'DLP', 'TABAQ', 'ANTEC IAM / Angina inestable', 'ANTEC BY PASS', 'ANTEC ATC', 'ANTEC ACE', 'ANTEC INS RENAL', 'ACV TIA PREV', 'EPOC', 'CLAUD ITTE', 'BB PREV', ' IECA/AT2  PREV', 'B CA PREV', 'AAS PREV', 'HIPOGLUCEMIANTES', 'DIURETICOS', 'INSULINA', 'ECG INFRA ST', 'ECG SUPRA ST', 'ECG INV T', 'BRI', 'BRD', 'ECG MCP', 'ECG FA', 'tropst', 'TnT Ultrasensible', 'Elevacion troponina T', 'AAS', 'BB', 'IECA', 'AT2', 'Clopi - prasu - tica', 'HEP sc o iv', 'IIb IIIa', 'B CA', 'ESTATINAS', 'HIPOGLUC METFORM', 'HIPOGL SULFAN', 'HIPOGL GLITAZ', 'INSULINA', 'INS + HIPOGL', 'INOTROPICOS', 'PRUEBA FUNC.', 'PRUEBA FUNC DE ALTO RIESGO', 'CCG', 'TRONCO', 'ATC PRIMARIA', 'ATC INTRAHOSP', 'TROMBOL', 'CRM', 'IAM HOSP(SI INTERNO POR ANGINA) O REIAM', 'APIAM', 'ANGINA REFRACT o RECURR', ' ACV/TIA', 'SANGRADO MAYOR*', 'I RENAL AGUDA']\n",
    "\n",
    "CATEGORY_FEATURES = CATEGORICAL_BINARY_FEATURES + CATEGORICAL_MULTI_CLASS_FEATURES\n",
    "\n",
    "TARGET = \"MUERTE HOSP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CATEGORY_FEATURES) + len(NUMERICAL_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer el split en train, validation y test. El porcentaje sera del 70% para train, 15% para validation y 15% para test. Esto nos deja con suficientes datos para entrenar y validar nuestros modelos.\n",
    "\n",
    "El resultado de este split va a ser 6 archivos. Dos archivos para train y test con X e y. Dos archivos para validation y test con X e y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=TARGET)\n",
    "y = df[[TARGET]]\n",
    "\n",
    "# First split: separate test set (15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Second split: from remaining 85%, get 70/15 ratio\n",
    "# To get 70% total from the 85% remaining, we need test_size = 15/85 ≈ 0.176\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=(15/85),  # This ensures we get 15% of total from the remaining 85%\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1818, 72)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1818\n",
      "Training samples: 1272 (70.0%)\n",
      "Validation samples: 273 (15.0%)\n",
      "Test samples: 273 (15.0%)\n"
     ]
    }
   ],
   "source": [
    "# Verify the proportions\n",
    "total_samples = len(X)\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Training samples: {len(X_train)} ({len(X_train)/total_samples:.1%})\")\n",
    "print(f\"Validation samples: {len(X_val)} ({len(X_val)/total_samples:.1%})\")\n",
    "print(f\"Test samples: {len(X_test)} ({len(X_test)/total_samples:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados en X_train: 0\n",
      "Duplicados en X_val: 0\n",
      "Duplicados en X_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Ver duplicados dentro de cada conjunto\n",
    "print(f\"Duplicados en X_train: {X_train.duplicated().sum()}\")\n",
    "print(f\"Duplicados en X_val: {X_val.duplicated().sum()}\")\n",
    "print(f\"Duplicados en X_test: {X_test.duplicated().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1272, 72)\n",
      "(273, 72)\n",
      "(273, 72)\n",
      "(1272, 1)\n",
      "(273, 1)\n",
      "(273, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New X_train shape: (1272, 72), y_train shape: (1272, 1)\n",
      "New X_val shape: (123, 72), y_val shape: (123, 1)\n",
      "New X_test shape: (105, 72), y_test shape: (105, 1)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates between X_train and X_val, and drop from y_val accordingly\n",
    "mask_val = ~X_val.isin(X_train.to_dict(orient='list')).all(axis=1)\n",
    "X_val = X_val[mask_val]\n",
    "y_val = y_val[mask_val]\n",
    "\n",
    "# Remove duplicates between X_train and X_test, and drop from y_test accordingly\n",
    "mask_test_train = ~X_test.isin(X_train.to_dict(orient='list')).all(axis=1)\n",
    "X_test = X_test[mask_test_train]\n",
    "y_test = y_test[mask_test_train]\n",
    "\n",
    "# Remove duplicates between X_val and X_test, and drop from y_test accordingly\n",
    "mask_test_val = ~X_test.isin(X_val.to_dict(orient='list')).all(axis=1)\n",
    "X_test = X_test[mask_test_val]\n",
    "y_test = y_test[mask_test_val]\n",
    "\n",
    "# Ensure no internal duplicates in each set\n",
    "X_train, y_train = X_train.drop_duplicates(), y_train.loc[X_train.index]\n",
    "X_val, y_val = X_val.drop_duplicates(), y_val.loc[X_val.index]\n",
    "X_test, y_test = X_test.drop_duplicates(), y_test.loc[X_test.index]\n",
    "\n",
    "# Print final dataset shapes\n",
    "print(f\"New X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"New X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"New X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDAD NaNs: 0 - 0.0%\n",
      "SEX0 NaNs: 0 - 0.0%\n",
      "HTA NaNs: 0 - 0.0%\n",
      "DBT NaNs: 0 - 0.0%\n",
      "DLP NaNs: 0 - 0.0%\n",
      "TABAQ NaNs: 0 - 0.0%\n",
      "ANTEC IAM / Angina inestable NaNs: 0 - 0.0%\n",
      "ANTEC BY PASS NaNs: 0 - 0.0%\n",
      "ANTEC ATC NaNs: 0 - 0.0%\n",
      "ANTEC ACE NaNs: 0 - 0.0%\n",
      "ANTEC INS RENAL NaNs: 0 - 0.0%\n",
      "ACV TIA PREV NaNs: 0 - 0.0%\n",
      "EPOC NaNs: 0 - 0.0%\n",
      "CLAUD ITTE NaNs: 0 - 0.0%\n",
      "BB PREV NaNs: 0 - 0.0%\n",
      " IECA/AT2  PREV NaNs: 0 - 0.0%\n",
      "B CA PREV NaNs: 0 - 0.0%\n",
      "AAS PREV NaNs: 0 - 0.0%\n",
      "HIPOGLUCEMIANTES NaNs: 0 - 0.0%\n",
      "DIURETICOS NaNs: 0 - 0.0%\n",
      "INSULINA NaNs: 0 - 0.0%\n",
      "PESO Kg NaNs: 93 - 5.115511551155116%\n",
      "ALTURA cm NaNs: 120 - 6.600660066006601%\n",
      "ECG INFRA ST NaNs: 0 - 0.0%\n",
      "ECG SUPRA ST NaNs: 0 - 0.0%\n",
      "ECG INV T NaNs: 0 - 0.0%\n",
      "BRI NaNs: 1 - 0.05500550055005501%\n",
      "BRD NaNs: 0 - 0.0%\n",
      "ECG MCP NaNs: 0 - 0.0%\n",
      "ECG FA NaNs: 1 - 0.05500550055005501%\n",
      "tropst NaNs: 2 - 0.11001100110011001%\n",
      "TnT Ultrasensible NaNs: 0 - 0.0%\n",
      "Elevacion troponina T NaNs: 0 - 0.0%\n",
      "CREAT NaNs: 10 - 0.5500550055005501%\n",
      "GLUCEMIA INGR NaNs: 83 - 4.565456545654565%\n",
      "GB NaNs: 66 - 3.6303630363036303%\n",
      "TAS INGRESO NaNs: 12 - 0.6600660066006601%\n",
      "FC INGRESO NaNs: 11 - 0.605060506050605%\n",
      "KILLIP Ingreso NaNs: 0 - 0.0%\n",
      "AAS NaNs: 0 - 0.0%\n",
      "BB NaNs: 18 - 0.9900990099009901%\n",
      "IECA NaNs: 0 - 0.0%\n",
      "AT2 NaNs: 1 - 0.05500550055005501%\n",
      "Clopi - prasu - tica NaNs: 0 - 0.0%\n",
      "HEP sc o iv NaNs: 0 - 0.0%\n",
      "IIb IIIa NaNs: 0 - 0.0%\n",
      "B CA NaNs: 1 - 0.05500550055005501%\n",
      "ESTATINAS NaNs: 0 - 0.0%\n",
      "HIPOGLUC METFORM NaNs: 1 - 0.05500550055005501%\n",
      "HIPOGL SULFAN NaNs: 0 - 0.0%\n",
      "HIPOGL GLITAZ NaNs: 4 - 0.22002200220022003%\n",
      "INSULINA.1 NaNs: 1 - 0.05500550055005501%\n",
      "INS + HIPOGL NaNs: 1 - 0.05500550055005501%\n",
      "INOTROPICOS NaNs: 18 - 0.9900990099009901%\n",
      "FUNCION VENTRICULAR IZQ NaNs: 8 - 0.44004400440044006%\n",
      "DIAGNOSTICO NaNs: 1 - 0.05500550055005501%\n",
      "PRUEBA FUNC. NaNs: 0 - 0.0%\n",
      "PRUEBA FUNC DE ALTO RIESGO NaNs: 1 - 0.05500550055005501%\n",
      "CCG NaNs: 0 - 0.0%\n",
      "TRONCO NaNs: 1 - 0.05500550055005501%\n",
      "NUMERO VASOS NaNs: 2 - 0.11001100110011001%\n",
      "ATC PRIMARIA NaNs: 0 - 0.0%\n",
      "ATC INTRAHOSP NaNs: 0 - 0.0%\n",
      "TROMBOL NaNs: 1 - 0.05500550055005501%\n",
      "CRM NaNs: 3 - 0.16501650165016502%\n",
      "MUERTE HOSP NaNs: 0 - 0.0%\n",
      "IAM HOSP(SI INTERNO POR ANGINA) O REIAM NaNs: 3 - 0.16501650165016502%\n",
      "APIAM NaNs: 3 - 0.16501650165016502%\n",
      "Peor KILLIP NaNs: 0 - 0.0%\n",
      "ANGINA REFRACT o RECURR NaNs: 3 - 0.16501650165016502%\n",
      " ACV/TIA NaNs: 4 - 0.22002200220022003%\n",
      "SANGRADO MAYOR* NaNs: 4 - 0.22002200220022003%\n",
      "I RENAL AGUDA NaNs: 2 - 0.11001100110011001%\n"
     ]
    }
   ],
   "source": [
    "print_column_with_nan(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PESO Kg NaNs: 93 - 5.115511551155116%\n",
      "ALTURA cm NaNs: 120 - 6.600660066006601%\n",
      "GLUCEMIA INGR NaNs: 83 - 4.565456545654565%\n",
      "GB NaNs: 66 - 3.6303630363036303%\n"
     ]
    }
   ],
   "source": [
    "print_column_with_nan(df, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer una imputación de los datos faltantes. Para las variables numericas, usamos medias. Para las categoricas, usamos moda.\n",
    "\n",
    "Elegimos este método ya que es el más sencillo y no afecta la distribución de las variables.\n",
    "\n",
    "Hacemos un fit de los datos de entrenamiento para obtener las medias y modas de las variables. Luego usamos el fit para imputar los datos faltantes de train, val y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "median_imputer.fit(X_train[NUMERICAL_FEATURES])\n",
    "mode_imputer.fit(X_train[CATEGORY_FEATURES])\n",
    "\n",
    "X_train_imputed = X_train.copy()\n",
    "X_val_imputed = X_val.copy()\n",
    "X_test_imputed = X_test.copy()\n",
    "\n",
    "X_train_imputed[NUMERICAL_FEATURES] = median_imputer.transform(X_train[NUMERICAL_FEATURES])\n",
    "X_train_imputed[CATEGORY_FEATURES] = mode_imputer.transform(X_train[CATEGORY_FEATURES])\n",
    "\n",
    "X_val_imputed[NUMERICAL_FEATURES] = median_imputer.transform(X_val[NUMERICAL_FEATURES])\n",
    "X_val_imputed[CATEGORY_FEATURES] = mode_imputer.transform(X_val[CATEGORY_FEATURES])\n",
    "\n",
    "X_test_imputed[NUMERICAL_FEATURES] = median_imputer.transform(X_test[NUMERICAL_FEATURES])\n",
    "X_test_imputed[CATEGORY_FEATURES] = mode_imputer.transform(X_test[CATEGORY_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = X_train_imputed.drop(columns=['INSULINA.1'])\n",
    "X_val_imputed = X_val_imputed.drop(columns=['INSULINA.1'])\n",
    "X_test_imputed = X_test_imputed.drop(columns=['INSULINA.1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos un escalado de las variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train_imputed.copy()\n",
    "X_val_scaled = X_val_imputed.copy()\n",
    "X_test_scaled = X_test_imputed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled[NUMERICAL_FEATURES] = scaler.fit_transform(X_train_imputed[NUMERICAL_FEATURES])\n",
    "X_val_scaled[NUMERICAL_FEATURES] = scaler.transform(X_val_imputed[NUMERICAL_FEATURES])\n",
    "X_test_scaled[NUMERICAL_FEATURES] = scaler.transform(X_test_imputed[NUMERICAL_FEATURES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save datasets\n",
    "\n",
    "Finalmelte guardamos los datasets en otra carpeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.to_csv('../data-new/train/neural_network/X_train_scaled-no_selection.csv', index=False)\n",
    "X_val_scaled.to_csv('../data-new/val/neural_network/X_val_scaled.csv-no_selection', index=False)\n",
    "X_test_scaled.to_csv('../data-new/test/neural_network/X_test_scaled-no_selection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv('../data-new/train/neural_network/y_train.csv', index=False)\n",
    "y_val.to_csv('../data-new/val/neural_network/y_val.csv', index=False)\n",
    "y_test.to_csv('../data-new/test/neural_network/y_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
