{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import print_column_with_nan, clean_value, get_columns_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data-new/data-selected-features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['I RENAL AGUDA', 'NUMERO VASOS', 'SANGRADO MAYOR*', 'FUNCION VENTRICULAR IZQ', 'CRM', 'CREAT', 'KILLIP Ingreso', 'GB', 'Peor KILLIP', 'INOTROPICOS', 'TAS INGRESO', 'GLUCEMIA INGR']\n",
    "\n",
    "NUMERICAL_FEATURES = ['CREAT', 'TAS INGRESO', 'GB', 'GLUCEMIA INGR']\n",
    "\n",
    "CATEGORICAL_MULTI_CLASS_FEATURES = ['NUMERO VASOS', 'FUNCION VENTRICULAR IZQ', 'KILLIP Ingreso', 'Peor KILLIP', ]\n",
    "CATEGORICAL_BINARY_FEATURES = ['I RENAL AGUDA', 'SANGRADO MAYOR*', 'CRM',  'INOTROPICOS',]\n",
    "\n",
    "CATEGORY_FEATURES = CATEGORICAL_BINARY_FEATURES + CATEGORICAL_MULTI_CLASS_FEATURES\n",
    "\n",
    "TARGET = \"MUERTE HOSP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I RENAL AGUDA NaNs: 2 - 0.11001100110011001%\n",
      "NUMERO VASOS NaNs: 2 - 0.11001100110011001%\n",
      "SANGRADO MAYOR* NaNs: 4 - 0.22002200220022003%\n",
      "FUNCION VENTRICULAR IZQ NaNs: 8 - 0.44004400440044006%\n",
      "CRM NaNs: 3 - 0.16501650165016502%\n",
      "CREAT NaNs: 10 - 0.5500550055005501%\n",
      "KILLIP Ingreso NaNs: 0 - 0.0%\n",
      "GB NaNs: 66 - 3.6303630363036303%\n",
      "Peor KILLIP NaNs: 0 - 0.0%\n",
      "INOTROPICOS NaNs: 18 - 0.9900990099009901%\n",
      "TAS INGRESO NaNs: 12 - 0.6600660066006601%\n",
      "GLUCEMIA INGR NaNs: 83 - 4.565456545654565%\n",
      "MUERTE HOSP NaNs: 0 - 0.0%\n"
     ]
    }
   ],
   "source": [
    "print_column_with_nan(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB NaNs: 66 - 3.6303630363036303%\n",
      "GLUCEMIA INGR NaNs: 83 - 4.565456545654565%\n"
     ]
    }
   ],
   "source": [
    "print_column_with_nan(df, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer una imputación de los datos faltantes. Para las variables numericas, usamos medias. Para las categoricas, usamos moda.\n",
    "\n",
    "Elegimos este método ya que es el más sencillo y no afecta la distribución de las variables.\n",
    "\n",
    "Hacemos un fit de los datos de entrenamiento para obtener las medias y modas de las variables. Luego usamos el fit para imputar los datos faltantes de train, val y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data-new/train/X_train.csv')\n",
    "y_train = pd.read_csv('../data-new/train/y_train.csv')\n",
    "X_val = pd.read_csv('../data-new/val/X_val.csv')\n",
    "y_val = pd.read_csv('../data-new/val/y_val.csv')\n",
    "X_test = pd.read_csv('../data-new/test/X_test.csv')\n",
    "y_test = pd.read_csv('../data-new/test/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "median_imputer.fit(X_train[NUMERICAL_FEATURES])\n",
    "mode_imputer.fit(X_train[CATEGORY_FEATURES])\n",
    "\n",
    "X_train_imputed = X_train.copy()\n",
    "X_val_imputed = X_val.copy()\n",
    "X_test_imputed = X_test.copy()\n",
    "\n",
    "X_train_imputed[NUMERICAL_FEATURES] = median_imputer.transform(X_train[NUMERICAL_FEATURES])\n",
    "X_train_imputed[CATEGORY_FEATURES] = mode_imputer.transform(X_train[CATEGORY_FEATURES])\n",
    "\n",
    "X_val_imputed[NUMERICAL_FEATURES] = median_imputer.transform(X_val[NUMERICAL_FEATURES])\n",
    "X_val_imputed[CATEGORY_FEATURES] = mode_imputer.transform(X_val[CATEGORY_FEATURES])\n",
    "\n",
    "X_test_imputed[NUMERICAL_FEATURES] = median_imputer.transform(X_test[NUMERICAL_FEATURES])\n",
    "X_test_imputed[CATEGORY_FEATURES] = mode_imputer.transform(X_test[CATEGORY_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed.to_csv('../data-new/train/X_train_imputed.csv', index=False)\n",
    "X_val_imputed.to_csv('../data-new/val/X_val_imputed.csv', index=False)\n",
    "X_test_imputed.to_csv('../data-new/test/X_test_imputed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imputación de los datos la vamos a hacer en dos partes. Para aquellas features que tienen valores faltantes mayores o iguales al 2% del total, imputamos mediante la técnica MICE (Multiple Imputation by Chained Equations). Para las features con valores faltantes menores a 2%, hacemos una imputación aleatoria ya que no afecta la distribución de la feature en particular."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
